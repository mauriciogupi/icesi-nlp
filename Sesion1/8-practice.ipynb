{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8QtoXwvzeZQ"
      },
      "source": [
        "# NLP Basics Assessment\n",
        "## Extracción de sentimiento de tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNG-UgJDzeZX"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cam2149/icesi-nlp/blob/main/Sesion1/8-practice.ipynb)\n",
        "\n",
        "En este notebook vamos a poner en práctica algunos de los conceptos vistos en los notebooks anteriores, aplicado a un corpus específico:\n",
        "\n",
        "Con tantos tuits circulando a cada segundo, es difícil determinar si el sentimiento detrás de un tuit específico impactará la marca de una empresa o persona por ser viral (positivo), o si devastará las ganancias por su tono negativo. Capturar el sentimiento con palabras es importante en estos tiempos donde las decisiones y reacciones se crean y actualizan en segundos. Pero, ¿qué palabras conducen realmente a la descripción del sentimiento? En esta competencia, tendrás que identificar la parte del tuit (palabra o frase) que refleje el sentimiento.\n",
        "\n",
        "\"Mi perro ridículo es increíble.\" [sentimiento: positivo]\n",
        "\n",
        "Desarrollar habilidades en esta importante área con este amplio conjunto de datos de tuits. Perfecciona tu técnica para alcanzar el primer puesto en esta competencia. ¿Qué palabras en los tuits respaldan un sentimiento positivo, negativo o neutral? ¿Cómo puedes ayudar a determinarlo usando herramientas de aprendizaje automático?\n",
        "\n",
        "El conjunto de datos se titula \"Análisis de Sentimiento: Emoción en Tweets de Texto con Etiquetas de Sentimiento existentes\", utilizado aquí bajo la licencia Creative Commons Atribución 4.0 Internacional. El objetivo en este concurso es construir un modelo que pueda hacer lo mismo: analizar el sentimiento etiquetado de un tweet determinado y determinar qué palabra o frase lo respalda mejor.\n",
        "\n",
        "Descargo de responsabilidad: El conjunto de datos de este concurso contiene texto que puede considerarse profano, vulgar u ofensivo.\n",
        "\n",
        "## Referencias\n",
        "* [Extracción de sentimiento de tweets](https://www.kaggle.com/competitions/tweet-sentiment-extraction/overview)\n",
        "* [NLP - Natural Language Processing With Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!pip install vaderSentiment\n",
        "!pip install tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt14wYva4ykp",
        "outputId": "f14f3a5b-7361-4d5b-d1a7-6d0fcf99ca4a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.8.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X_TYq6mbzeZZ",
        "outputId": "741368fe-e850-4de3-99f1-9ab0535779ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1741755694.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "import warnings\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from spacy.matcher import Matcher\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import os\n",
        "from google.colab import files\n",
        "from tqdm import tqdm  # Importa tqdm para la barra de progreso\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "installed_packages = [package.key for package in pkg_resources.working_set]\n",
        "IN_COLAB = 'google-colab' in installed_packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBWsGdVvnZ-i",
        "outputId": "0fc210d9-a618-4041-bf89-908fb2f1c459"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-trf==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-curated-transformers<1.0.0,>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-trf==3.8.0) (0.3.1)\n",
            "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.1.1)\n",
            "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.0.9)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.2.0)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.11/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load the Kaggle dataset conditionally\n",
        "%%bash\n",
        "\n",
        "if [ ! -d \"tweet_data\" ]; then\n",
        "  echo \"Downloading and extracting dataset...\"\n",
        "  mkdir -p ~/.kaggle\n",
        "  # Assuming kaggle.json is already uploaded to Colab's files\n",
        "  test -f \"kaggle.json\" && mv kaggle.json ~/.kaggle/\n",
        "  chmod 600 ~/.kaggle/kaggle.json\n",
        "  kaggle competitions download -c tweet-sentiment-extraction\n",
        "  unzip -o tweet-sentiment-extraction.zip -d tweet_data\n",
        "else\n",
        "  echo \"Dataset already exists in tweet_data directory.\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxX3d32poN-x",
        "outputId": "110b4b10-0995-403b-9fc4-895ca7f763ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already exists in tweet_data directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!test '{IN_COLAB}' = 'True' && wget -O requirements.txt https://github.com/cam2149/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiN-94JxnXtn",
        "outputId": "d12528ec-380c-4a94-ce87-14e77b036306"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-09 20:23:59--  https://github.com/cam2149/icesi-nlp/raw/refs/heads/main/requirements.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/cam2149/icesi-nlp/refs/heads/main/requirements.txt [following]\n",
            "--2025-08-09 20:23:59--  https://raw.githubusercontent.com/cam2149/icesi-nlp/refs/heads/main/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 349 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     349  --.-KB/s    in 0s      \n",
            "\n",
            "2025-08-09 20:23:59 (19.0 MB/s) - ‘requirements.txt’ saved [349/349]\n",
            "\n",
            "Requirement already satisfied: pandas==2.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.1.1)\n",
            "Requirement already satisfied: matplotlib==3.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.8.0)\n",
            "Requirement already satisfied: seaborn==0.12.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.12.2)\n",
            "Requirement already satisfied: scikit-learn==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: statsmodels==0.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.14.0)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.66.1)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.2.0)\n",
            "Requirement already satisfied: lightning==2.2.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (2.2.0.post0)\n",
            "Requirement already satisfied: tensorboard==2.16.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.16.2)\n",
            "Requirement already satisfied: bokeh==3.3.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (3.3.4)\n",
            "Requirement already satisfied: transformers==4.41.2 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]==4.41.2->-r requirements.txt (line 11)) (4.41.2)\n",
            "Requirement already satisfied: datasets==2.19.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.19.1)\n",
            "Requirement already satisfied: torchinfo==1.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.8.0)\n",
            "Requirement already satisfied: accelerate==0.30.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.30.1)\n",
            "Requirement already satisfied: evaluate==0.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.4.2)\n",
            "Requirement already satisfied: sentence-transformers==3.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (3.0.1)\n",
            "Requirement already satisfied: gradio==4.36.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.36.1)\n",
            "Requirement already satisfied: ollama==0.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (0.2.1)\n",
            "Requirement already satisfied: spacy==3.8.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (3.8.7)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 4)) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 4)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from statsmodels==0.14.0->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (2.2.0)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning==2.2.0.post0->-r requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from lightning==2.2.0.post0->-r requirements.txt (line 8)) (0.15.2)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning==2.2.0.post0->-r requirements.txt (line 8)) (1.8.1)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning==2.2.0.post0->-r requirements.txt (line 8)) (2.5.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (1.74.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (3.8.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (3.1.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.11/dist-packages (from bokeh==3.3.4->-r requirements.txt (line 10)) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh==3.3.4->-r requirements.txt (line 10)) (2025.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (0.34.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (0.6.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (3.12.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.30.1->-r requirements.txt (line 14)) (5.9.5)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.0.1 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (1.0.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.27.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (6.5.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (3.11.1)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.12.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.16.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (2.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.35.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (0.4.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1->-r requirements.txt (line 20)) (8.2.1)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.0.1->gradio==4.36.1->-r requirements.txt (line 17)) (11.0.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->-r requirements.txt (line 7)) (12.4.127)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (2.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (1.1.7)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.8.7->-r requirements.txt (line 19)) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.36.1->-r requirements.txt (line 17)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (3.4.2)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy==3.8.7->-r requirements.txt (line 19)) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy==3.8.7->-r requirements.txt (line 19)) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.7->-r requirements.txt (line 19)) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.7->-r requirements.txt (line 19)) (7.3.0.post1)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->gradio==4.36.1->-r requirements.txt (line 17)) (0.47.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.26.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.8.7->-r requirements.txt (line 19)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.8.7->-r requirements.txt (line 19)) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QTAGcSDXzeZd"
      },
      "outputs": [],
      "source": [
        "# Initialize SpaCy and VADER\n",
        "#Esta celda de código inicializa las dos bibliotecas principales utilizadas en este notebook para el procesamiento del lenguaje natural y el análisis de sentimiento: SpaCy y VADER.\n",
        "\n",
        "#nlp = spacy.load(\"en_core_web_sm\"): Esta línea carga un modelo de lenguaje inglés pre-entrenado de la biblioteca SpaCy. El modelo \"en_core_web_sm\" es un modelo pequeño de inglés que incluye capacidades para tokenización, etiquetado de parte de la oración (POS tagging), análisis de dependencias y más. Este modelo cargado se asigna a la variable nlp, que luego se utiliza para procesar texto en todo el notebook.\n",
        "\n",
        "#analyzer = SentimentIntensityAnalyzer(): Esta línea crea una instancia del SentimentIntensityAnalyzer de la biblioteca VADER (Valence Aware Dictionary and sEntiment Reasoner). VADER es una herramienta de análisis de sentimiento basada en léxico y reglas que está específicamente sintonizada con los sentimientos expresados en las redes sociales. El objeto analizador creado se asigna a la variable analyzer, que se utilizará más adelante para obtener puntuaciones de sentimiento para el texto.\n",
        "\n",
        "#En esencia, esta celda configura las herramientas necesarias (SpaCy para el procesamiento lingüístico y VADER para la puntuación de sentimiento) para analizar los datos de texto en los tweets.\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "analyzer = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "try:\n",
        "    train_df = pd.read_csv(\"tweet_data/train.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: train.csv not found even after attempting download and extraction.\")\n",
        "    # You might want to add code here to handle the case where the file is still not found.\n",
        "\n",
        "\n",
        "# Data Exploration\n",
        "print(\"Dataset Preview:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nColumns:\", train_df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaT6zxFu12sE",
        "outputId": "f429601e-505e-4124-80e9-88da93d5e419"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Preview:\n",
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
            "\n",
            "                         selected_text sentiment  \n",
            "0  I`d have responded, if I were going   neutral  \n",
            "1                             Sooo SAD  negative  \n",
            "2                          bullying me  negative  \n",
            "3                       leave me alone  negative  \n",
            "4                        Sons of ****,  negative  \n",
            "\n",
            "Columns: ['textID', 'text', 'selected_text', 'sentiment']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "4QePzqgb2dcz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count initial number of rows\n",
        "initial_rows = len(train_df)\n",
        "# Filter out rows containing either \" ****\" or \"http\"\n",
        "train_df = train_df[~train_df['text'].astype(str).str.contains(r\" \\*\\*\\*\\*|http\", regex=True)]\n",
        "# Count remaining rows\n",
        "rows_after_removal = len(train_df)\n",
        "# Display results\n",
        "print(f\"Removed {initial_rows - rows_after_removal} rows containing ' ****' or 'http'.\")\n",
        "print(f\"Remaining rows: {rows_after_removal}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bYJTKG9Qk49",
        "outputId": "6ddfcc74-3f36-43f1-d6c3-76f505b5f3c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 2066 rows containing ' ****' or 'http'.\n",
            "Remaining rows: 25415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of tokens in the processed_text column\n",
        "# Handle potential non-string values by converting them to strings and replacing NaN with empty strings\n",
        "token_count = train_df[\"text\"].astype(str).apply(lambda x: len(x.split())).sum()\n",
        "\n",
        "print(f\"\\nTotal number of tokens in the dataset: {token_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZytBFfd_8a73",
        "outputId": "5ba449ea-76c5-4ec6-d8c0-6abc46a2dc6b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of tokens in the dataset: 326105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Get the number of records in the DataFrame\n",
        "num_records = len(train_df)\n",
        "# Generate a random integer between 0 and num_records-1 (inclusive)\n",
        "random_index = random.randint(0, num_records - 1)\n",
        "print(f\"A random index based on the number of records is: {random_index}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZKxY-pNxXis",
        "outputId": "3f039588-3c78-4e4d-9ed4-643af1733c71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A random index based on the number of records is: 1579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a row from the dataset (e.g., the first row)\n",
        "selected_row = train_df.iloc[random_index]\n",
        "text = selected_row[\"text\"]\n",
        "# Process the text with SpaCy\n",
        "doc = nlp(text)\n",
        "# Print information for each token\n",
        "print(f\"Analyzing row {random_index} in the dataset:\\n\")\n",
        "print(f\"Row Info:\\n{selected_row}\\n\") # Corrected line\n",
        "print(f\"Analyzing text: '{text}'\\n\")\n",
        "print(\"{:20}{:20}{:20}{:20}\".format(\"Text\", \"POS\", \"dep\", \"lemma\"))\n",
        "for token in doc:\n",
        "    print(f\"{token.text:{20}}{token.pos_:{20}}{token.dep_:{20}}{token.lemma_:{20}}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUDsZRIO-vf4",
        "outputId": "72eb0b2f-77cd-4492-af4f-2bf9fa0ab67f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing row 1579 in the dataset:\n",
            "\n",
            "Row Info:\n",
            "textID                                                  a90a8fe617\n",
            "text              I won`t b hereeeeee. Imma b in georgia  and t...\n",
            "selected_text    I won`t b hereeeeee. Imma b in georgia  and th...\n",
            "sentiment                                                  neutral\n",
            "Name: 1690, dtype: object\n",
            "\n",
            "Analyzing text: ' I won`t b hereeeeee. Imma b in georgia  and then I`m off to LA. Grrrrrr!'\n",
            "\n",
            "Text                POS                 dep                 lemma               \n",
            "                    SPACE               dep                                     \n",
            "I                   PRON                nsubj               I                   \n",
            "won`t               AUX                 aux                 won`t               \n",
            "b                   VERB                pcomp               b                   \n",
            "hereeeeee           INTJ                intj                hereeeeee           \n",
            ".                   PUNCT               punct               .                   \n",
            "Imma                PROPN               compound            Imma                \n",
            "b                   NOUN                nmod                b                   \n",
            "in                  ADP                 prep                in                  \n",
            "georgia             PROPN               pobj                georgia             \n",
            "                    SPACE               dep                                     \n",
            "and                 CCONJ               cc                  and                 \n",
            "then                ADV                 advmod              then                \n",
            "I`m                 VERB                conj                i`m                 \n",
            "off                 ADP                 prep                off                 \n",
            "to                  ADP                 prep                to                  \n",
            "LA                  PROPN               pobj                LA                  \n",
            ".                   PUNCT               punct               .                   \n",
            "Grrrrrr             INTJ                ROOT                grrrrrr             \n",
            "!                   PUNCT               punct               !                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of sentences in the text column\n",
        "# Handle potential non-string values by converting them to strings and replacing NaN with empty strings\n",
        "sentence_count =train_df.iloc[random_index].astype(str).apply(lambda x: len(list(nlp(x).sents))).sum()\n",
        "\n",
        "print(f\"\\nTotal number of sentences in the selectext: {sentence_count}\")"
      ],
      "metadata": {
        "id": "sX5EKGKq9PNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808f7fd2-93b7-4a28-be82-1355e892941b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of sentences in the selectext: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp(text)\n",
        "# dep for syntactic dependency\n",
        "# Este código utiliza displacy para visualizar las dependencias sintácticas de una oración.\n",
        "# 'doc' es el objeto Doc de spaCy que contiene la oración procesada.\n",
        "# style='dep' especifica que se visualicen las dependencias.\n",
        "# jupyter=True permite renderizar la visualización directamente en un entorno Jupyter o Colab.\n",
        "# options={'distance': 110} ajusta la distancia entre los tokens en la visualización para mejorar la legibilidad.\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 110})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "Rmyj32qZbt5n",
        "outputId": "e29f5baf-f45d-47e9-fdd1-b6fff1a608e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f332fecde28b42d28a44eb1520d3adc5-0\" class=\"displacy\" width=\"1920\" height=\"357.0\" direction=\"ltr\" style=\"max-width: none; height: 357.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\"> </tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">SPACE</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"160\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"160\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"270\">won`t</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"270\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"380\">b</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"380\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"490\">hereeeeee.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"490\">INTJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"600\">Imma</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"600\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"710\">b</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"710\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">georgia</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\"> </tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">SPACE</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1260\">then</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1260\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1370\">I`m</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1370\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1480\">off</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1480\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1590\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1590\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1700\">LA.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1700\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1810\">Grrrrrr!</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1810\">INTJ</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-0\" stroke-width=\"2px\" d=\"M180,222.0 C180,112.0 370.0,112.0 370.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M180,224.0 L172,212.0 188,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-1\" stroke-width=\"2px\" d=\"M290,222.0 C290,167.0 365.0,167.0 365.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M290,224.0 L282,212.0 298,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-2\" stroke-width=\"2px\" d=\"M70,222.0 C70,57.0 375.0,57.0 375.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M375.0,224.0 L383.0,212.0 367.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-3\" stroke-width=\"2px\" d=\"M400,222.0 C400,167.0 475.0,167.0 475.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">intj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M475.0,224.0 L483.0,212.0 467.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-4\" stroke-width=\"2px\" d=\"M620,222.0 C620,167.0 695.0,167.0 695.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M620,224.0 L612,212.0 628,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-5\" stroke-width=\"2px\" d=\"M70,222.0 C70,2.0 710.0,2.0 710.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M710.0,224.0 L718.0,212.0 702.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-6\" stroke-width=\"2px\" d=\"M730,222.0 C730,167.0 805.0,167.0 805.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M805.0,224.0 L813.0,212.0 797.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-7\" stroke-width=\"2px\" d=\"M840,222.0 C840,167.0 915.0,167.0 915.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,224.0 L923.0,212.0 907.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-8\" stroke-width=\"2px\" d=\"M730,222.0 C730,112.0 1030.0,112.0 1030.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1030.0,224.0 L1038.0,212.0 1022.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-9\" stroke-width=\"2px\" d=\"M730,222.0 C730,57.0 1145.0,57.0 1145.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1145.0,224.0 L1153.0,212.0 1137.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-10\" stroke-width=\"2px\" d=\"M1280,222.0 C1280,167.0 1355.0,167.0 1355.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1280,224.0 L1272,212.0 1288,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-11\" stroke-width=\"2px\" d=\"M730,222.0 C730,2.0 1370.0,2.0 1370.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1370.0,224.0 L1378.0,212.0 1362.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-12\" stroke-width=\"2px\" d=\"M1390,222.0 C1390,167.0 1465.0,167.0 1465.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1465.0,224.0 L1473.0,212.0 1457.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-13\" stroke-width=\"2px\" d=\"M1500,222.0 C1500,167.0 1575.0,167.0 1575.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1575.0,224.0 L1583.0,212.0 1567.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f332fecde28b42d28a44eb1520d3adc5-0-14\" stroke-width=\"2px\" d=\"M1610,222.0 C1610,167.0 1685.0,167.0 1685.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f332fecde28b42d28a44eb1520d3adc5-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1685.0,224.0 L1693.0,212.0 1677.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Importa tqdm para la barra de progreso\n",
        "\n",
        "# Asegúrate de envolver la función con tqdm\n",
        "tqdm.pandas(desc=\"Processing Text Justification\")\n",
        "\n",
        "# Text Justification Extraction with SpaCy Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define patrones positivos y negativos\n",
        "positive_pattern = [{\"LOWER\": {\"IN\": [\"good\", \"great\", \"excellent\", \"love\", \"amazing\"]}}]\n",
        "negative_pattern = [{\"LOWER\": {\"IN\": [\"bad\", \"poor\", \"terrible\", \"hate\", \"sad\", \"bullying\",\n",
        "                                     \"leave me alone\", \"sons of\", \"son of\", \"boring\", \"aggressive\",\n",
        "                                     \"anxiety\", \"angst\", \"gross\", \"chaos\", \"collapse\", \"confusion\",\n",
        "                                     \"cringe\", \"critical\", \"damage\", \"disappointment\", \"deficient\",\n",
        "                                     \"unpleasant\", \"disastrous\", \"desperate\", \"disillusion\", \"pain\",\n",
        "                                     \"sick\", \"angry\", \"error\", \"stupid\", \"failure\", \"frustration\",\n",
        "                                     \"horrible\", \"unacceptable\", \"incompetent\", \"ineffective\", \"unfair\",\n",
        "                                     \"slow\", \"bad\", \"awful\", \"annoying\", \"negative\", \"danger\", \"loss\",\n",
        "                                     \"problem\", \"rejection\", \"ridiculous\", \"risky\", \"terrible\", \"toxic\",\n",
        "                                     \"shame\", \"wtf\", \"fail\", \"ew\", \"meh\", \"so gross\", \"nooo\", \"ugh\",\n",
        "                                     \"lame\", \"trash\", \"cancelled\", \"cancel him\", \"cancel her\", \"worst\",\n",
        "                                     \"fatal\", \"disgusting\", \"why tho\", \"nah\", \"not cool\", \"dead\",\n",
        "                                     \"over it\", \"fake\", \"phony\", \"drama\", \"messy\", \"leave me alone\",\n",
        "                                     \"sons of\", \"son of\"]}}]\n",
        "\n",
        "matcher.add(\"PositiveWords\", [positive_pattern])\n",
        "matcher.add(\"NegativeWords\", [negative_pattern])\n",
        "\n",
        "# Función para extraer la justificación de cada texto\n",
        "def extract_justification(text):\n",
        "    if isinstance(text, str):  # Asegurarse de que el texto sea una cadena\n",
        "        doc = nlp(text)\n",
        "        matches = matcher(doc)\n",
        "        if matches:\n",
        "            match_id, start, end = matches[0]\n",
        "            return doc[start:end].text\n",
        "    return \"\"\n",
        "\n",
        "# Aplica la función de justificación al DataFrame con tqdm para mostrar progreso\n",
        "train_df[\"extracted_text\"] = train_df[\"selected_text\"].progress_apply(extract_justification)\n",
        "\n",
        "print(\"\\nText justification extraction complete!\")\n"
      ],
      "metadata": {
        "id": "CUlvgfyl8QLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e080cf7-d281-465f-b011-200af19a650c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Text Justification: 100%|██████████| 25415/25415 [43:56<00:00,  9.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text justification extraction complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Exploration\n",
        "print(\"Dataset Preview:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nColumns:\", train_df.columns.tolist())"
      ],
      "metadata": {
        "id": "dzyMfKXJklp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function using SpaCy\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    doc = nlp(str(text))\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "train_df[\"processed_text\"] = train_df[\"selected_text\"].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "nN9o7q2iMTT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21100acf",
        "outputId": "f2259f3a-2364-4bb3-b14c-f459fce34abe"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Importa tqdm para la barra de progreso\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Inicializa el analizador VADER\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Función para obtener las puntuaciones de sentimiento\n",
        "def analyze_sentiment_scores(text):\n",
        "    if isinstance(text, str):  # Verifica que el texto sea una cadena\n",
        "        return analyzer.polarity_scores(text)\n",
        "    else:\n",
        "        return {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
        "\n",
        "# Añadimos la barra de progreso a la operación apply\n",
        "tqdm.pandas(desc=\"Applying VADER Sentiment Analysis\")\n",
        "\n",
        "# Aplicar la función de análisis de sentimiento con barra de progreso\n",
        "sentiment_scores_df = train_df['text'].progress_apply(analyze_sentiment_scores).apply(pd.Series)\n",
        "\n",
        "# Concatenar las nuevas columnas al DataFrame original\n",
        "train_df = pd.concat([train_df, sentiment_scores_df], axis=1)\n",
        "\n",
        "# Mostrar un vistazo de los primeros registros con las nuevas columnas de puntuaciones\n",
        "print(\"Dataset Preview with VADER Scores:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Imprimir las columnas nuevas añadidas\n",
        "print(\"\\nColumns:\", train_df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_vcjcd_ckOa",
        "outputId": "535db584-b9d2-44b7-8d3e-6aaf752114b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Applying VADER Sentiment Analysis: 100%|██████████| 25415/25415 [00:04<00:00, 6205.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Preview with VADER Scores:\n",
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
            "\n",
            "                         selected_text sentiment extracted_text    neg    neu  \\\n",
            "0  I`d have responded, if I were going   neutral                 0.000  1.000   \n",
            "1                             Sooo SAD  negative            SAD  0.474  0.526   \n",
            "2                          bullying me  negative       bullying  0.494  0.506   \n",
            "3                       leave me alone  negative                 0.538  0.462   \n",
            "6                                  fun  positive                 0.000  0.652   \n",
            "\n",
            "     pos  compound  \n",
            "0  0.000    0.0000  \n",
            "1  0.000   -0.7437  \n",
            "2  0.000   -0.5994  \n",
            "3  0.000   -0.3595  \n",
            "6  0.348    0.7506  \n",
            "\n",
            "Columns: ['textID', 'text', 'selected_text', 'sentiment', 'extracted_text', 'neg', 'neu', 'pos', 'compound']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict sentiment based on VADER compound score\n",
        "# Define thresholds for sentiment prediction\n",
        "def predict_vader_sentiment(compound_score):\n",
        "    if compound_score >= 0.05:\n",
        "        return 'positive'\n",
        "    elif compound_score <= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "train_df['predicted_sentiment'] = train_df['compound'].apply(predict_vader_sentiment)\n",
        "\n",
        "# Evaluation against provided labels\n",
        "print(\"\\nSentiment Prediction Evaluation:\")\n",
        "accuracy = accuracy_score(train_df[\"sentiment\"], train_df[\"predicted_sentiment\"])\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(classification_report(train_df[\"sentiment\"], train_df[\"predicted_sentiment\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GJ_ELYmA9kM",
        "outputId": "cbbb2106-6c08-4fce-ded3-c6495097a330"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentiment Prediction Evaluation:\n",
            "Accuracy: 0.6324\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.69      0.60      0.64      7096\n",
            "     neutral       0.71      0.47      0.57     10327\n",
            "    positive       0.56      0.87      0.68      7992\n",
            "\n",
            "    accuracy                           0.63     25415\n",
            "   macro avg       0.65      0.65      0.63     25415\n",
            "weighted avg       0.66      0.63      0.62     25415\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "print(\"\\nSample Results with Extracted Justification:\")\n",
        "print(train_df[[\"text\", \"sentiment\", \"predicted_sentiment\", \"extracted_text\"]].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sTLpiXrClFt",
        "outputId": "e8328ab5-e51f-4e40-896b-558ccfd80161"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Results with Extracted Justification:\n",
            "                                                text sentiment  \\\n",
            "0                I`d have responded, if I were going   neutral   \n",
            "1      Sooo SAD I will miss you here in San Diego!!!  negative   \n",
            "2                          my boss is bullying me...  negative   \n",
            "3                     what interview! leave me alone  negative   \n",
            "6  2am feedings for the baby are fun when he is a...  positive   \n",
            "\n",
            "  predicted_sentiment extracted_text  \n",
            "0             neutral                 \n",
            "1            negative            SAD  \n",
            "2            negative       bullying  \n",
            "3            negative                 \n",
            "6            positive                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.columns)\n",
        "print([type(col) for col in train_df.columns])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGWSDb3qeVuI",
        "outputId": "7657ef48-9467-4156-ef28-faf487cfa133"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['textID', 'text', 'selected_text', 'sentiment', 'extracted_text', 'neg',\n",
            "       'neu', 'pos', 'compound', 'predicted_sentiment'],\n",
            "      dtype='object')\n",
            "[<class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d53860eb",
        "outputId": "01a4dc69-6de3-4962-d4b2-4302caac81b6"
      },
      "source": [
        "# Calculate the percentage of rows where 'sentiment' and 'predicted_sentiment' are equal\n",
        "percentage_equal = (train_df['sentiment'] == train_df['predicted_sentiment']).mean() * 100\n",
        "\n",
        "print(f\"Percentage of rows where 'sentiment' and 'predicted_sentiment' are equal: {percentage_equal:.2f}%\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of rows where 'sentiment' and 'predicted_sentiment' are equal: 63.24%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}