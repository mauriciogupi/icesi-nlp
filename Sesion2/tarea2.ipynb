{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d4efb44e",
      "metadata": {
        "id": "d4efb44e"
      },
      "source": [
        "# NLP con Long-Short Term Memory (LSTM)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion2/2-nlp-with-lstm.ipynb)\n",
        "\n",
        "En este notebook implementaremos un clasificador de noticias en español utilizando la arquitectura de red LSTM. La idea es tener un punto de referencia para comparar cuando observemos la parte de transformers, por lo que utilizaremos el mismo dataset y tarea de ejemplo. Utilizarémos las utilidades de tokenización de huggingface transformers para ayudarnos con esta tarea.\n",
        "\n",
        "#### Referencias\n",
        "- Dataset: https://huggingface.co/datasets/MarcOrfilaCarreras/spanish-news\n",
        "- [Long Short-Term Memory](https://www.researchgate.net/publication/13853244_Long_Short-Term_Memory#fullTextFileContent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "936855e4",
      "metadata": {
        "id": "936855e4"
      },
      "outputs": [],
      "source": [
        "import pkg_resources\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "installed_packages = [package.key for package in pkg_resources.working_set]\n",
        "IN_COLAB = 'google-colab' in installed_packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "2013edaf",
      "metadata": {
        "id": "2013edaf",
        "outputId": "f8d7d901-5e19-4ae5-acfd-db7dddafbbab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connected to cloud.r-pr\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rHit:3 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 128 kB in 3s (38.1 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'python3-distutils' instead of 'python3.10-distutils'\n",
            "Note, selecting 'python3-lib2to3' instead of 'python3.10-lib2to3'\n",
            "python3-distutils is already the newest version (3.10.8-1~22.04).\n",
            "python3-lib2to3 is already the newest version (3.10.8-1~22.04).\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "update-alternatives: warning: forcing reinstallation of alternative /usr/bin/python3.11 because link group python is broken\n",
            "update-alternatives: warning: not replacing /usr/local/bin/python with a link\n",
            "update-alternatives: warning: forcing reinstallation of alternative /usr/bin/python3.11 because link group python is broken\n",
            "update-alternatives: warning: not replacing /usr/local/bin/python with a link\n",
            "Requirement already satisfied: lightning in /usr/local/lib/python3.11/dist-packages (2.5.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.11/dist-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2025.3.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (0.15.2)\n",
            "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (25.0)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchmetrics<3.0,>0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (1.8.1)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>4.5.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.14.1)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning) (2.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "#!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt\n",
        "!test '{IN_COLAB}' = 'True' && sudo apt-get update -y\n",
        "!test '{IN_COLAB}' = 'True' && sudo apt-get install python3.10 python3.10-distutils python3.10-lib2to3 -y\n",
        "!test '{IN_COLAB}' = 'True' && sudo update-alternatives --install /usr/local/bin/python python /usr/bin/python3.11 2\n",
        "!test '{IN_COLAB}' = 'True' && sudo update-alternatives --install /usr/local/bin/python python /usr/bin/python3.10 1\n",
        "!test '{IN_COLAB}' = 'True' && pip install lightning datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df40a98",
      "metadata": {
        "id": "6df40a98"
      },
      "source": [
        "### Cargando el dataset\n",
        "Este es un dataset pequeño de articulos de noticias en idioma español con sus respectivas categorías. El dataset está disponible en el HuggingFace Hub y puede ser fácilmente descargado con la librería."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "02b91601",
      "metadata": {
        "id": "02b91601",
        "outputId": "95d05a40-5275-43ed-c959-c7a429c63255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'timestamp', 'pais', 'ciudad', 'canal', 'plataforma', 'medio_contacto', 'comentario', 'categoria', 'severidad', 'sentimiento'],\n",
            "    num_rows: 2000\n",
            "})\n",
            "{'id': '3c0c82e9-c97d-421a-adda-b0e246fdb795', 'timestamp': Timestamp('2024-08-17 14:06:08'), 'pais': 'CL', 'ciudad': 'Lima', 'canal': 'telefono', 'plataforma': 'mobile_web', 'medio_contacto': 'redes', 'comentario': 'En la web/app el cupón no funcionó al comprar un libro. Intenté varias veces y nada.', 'categoria': 'ecommerce', 'severidad': 'media', 'sentimiento': -0.5}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "from datasets import load_dataset\n",
        "train_url = \"https://raw.githubusercontent.com/mauriciogupi/nlp-master/main/Sesion2/comentarios_train.json\"\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=train_url, split=\"train\")\n",
        "print(dataset)\n",
        "print(dataset[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a53ebd",
      "metadata": {
        "id": "f9a53ebd"
      },
      "source": [
        "Observemos uno de sus registros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "14abea7d",
      "metadata": {
        "id": "14abea7d",
        "outputId": "01e86bec-bfce-453e-d010-fa1921939e07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '4ecd2f26-8498-460e-8a6d-32a2c281e205',\n",
              " 'timestamp': Timestamp('2024-08-17 14:43:08'),\n",
              " 'pais': 'ES',\n",
              " 'ciudad': 'Buenos Aires',\n",
              " 'canal': 'whatsapp',\n",
              " 'plataforma': 'desktop',\n",
              " 'medio_contacto': 'telefono',\n",
              " 'comentario': 'El libro mostraba un precio y cobró otro; el descuento era de 20% según la página.',\n",
              " 'categoria': 'precios',\n",
              " 'severidad': 'media',\n",
              " 'sentimiento': -0.7000000000000001}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(lambda x: {\"timestamp\": str(x[\"timestamp\"])})"
      ],
      "metadata": {
        "id": "Npwnr-SbAEWP",
        "outputId": "6cefa9ab-842f-4db1-d443-5edcfb43814d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e667ffdde87c44e2b47e9122bd72ebba",
            "a144b0f65a6d48449b4b8dfca55d48ae",
            "d14fadcf268c44cc979bfd3b81ccb4dd",
            "0ea48a41f0c64ddb8713ec95c7fd9afe",
            "5ed5416f52c64c2ba004117116afbb94",
            "7f391260a25940f7bd12504517d095f6",
            "66b4f091239349bfacf1e28a39234486",
            "4091367ab02544acb7b79a59e6215087",
            "6bb0996639f4407d87bcf3d5769a3726",
            "67953ebe4e434630a3668706284889ae",
            "90d9c7c2328d48af8ee00a5d43da2ea0"
          ]
        }
      },
      "id": "Npwnr-SbAEWP",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e667ffdde87c44e2b47e9122bd72ebba"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a42e2676",
      "metadata": {
        "id": "a42e2676"
      },
      "source": [
        "Para los efectos de esta tarea, nos servirán el texto y la categoría naturalmente.\n",
        "\n",
        "A manera general, observemos que tan largos o cortos tienden a ser los textos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "c6642761",
      "metadata": {
        "id": "c6642761",
        "outputId": "91f46aeb-0bdd-4dcc-af37-d8251bc9429f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto más corto: 50\n",
            "Texto más largo: 104\n",
            "Longitud promedio: 83.0965\n"
          ]
        }
      ],
      "source": [
        "text_lengths = [len(row['comentario']) for row in dataset]\n",
        "print(f\"Texto más corto: {min(text_lengths)}\")\n",
        "print(f\"Texto más largo: {max(text_lengths)}\")\n",
        "print(f\"Longitud promedio: {sum(text_lengths) / len(text_lengths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19905224",
      "metadata": {
        "id": "19905224"
      },
      "source": [
        "Estos valores son la cantidad de *caractéres* que tiene las secuencias. Una decisión ingenua pero útil en este momento podría ser ajustar la longitud de las secuencias que vamos a usar para el entrenamiento a unos 2000 tokens. Esto podría ser suficiente para capturar una porción significativa de los textos.\n",
        "\n",
        "## Definiendo el Tokenizer\n",
        "\n",
        "Ahora, vamos a definir el tokenizer para nuestra tarea. Para mantener las cosas simples, vamos a mantener un conteo de palabras y vamos a hacer un corte hasta los primeros 50mil tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "18d840a7",
      "metadata": {
        "id": "18d840a7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def simple_tokenizer(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-záéíóúüñ]+\", \" \", text)\n",
        "    return text.strip().split()\n",
        "\n",
        "# Construimos el vocabulario a partir de conjunto de datos.\n",
        "token_counts = Counter()\n",
        "for text in dataset[\"comentario\"]:\n",
        "    token_counts.update(simple_tokenizer(text))\n",
        "\n",
        "# 50k-2 porque necesitamos reservar espacio para los dos tokens especiales\n",
        "top_n_tokens = list(token_counts.keys())[:50000-2]\n",
        "vocab = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
        "for token in top_n_tokens:\n",
        "    vocab[token] = len(vocab)\n",
        "\n",
        "def tokenize_text(text, max_length=50):\n",
        "    tokens = simple_tokenizer(text)\n",
        "    ids = [vocab.get(tok, vocab[\"[UNK]\"]) for tok in tokens[:max_length]]\n",
        "    ids += [vocab[\"[PAD]\"]] * (max_length - len(ids))\n",
        "    return ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e7ce90e",
      "metadata": {
        "id": "9e7ce90e"
      },
      "source": [
        "Exploremos ahora el tokenizador obtenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "9f037f43",
      "metadata": {
        "id": "9f037f43",
        "outputId": "ac4d0ce3-664d-4c4b-b9a3-22554be9562d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario: 313 tokens\n",
            "Primeros 15 tokens:\n",
            "['en', 'la', 'web', 'app', 'el', 'cupón', 'no', 'funcionó', 'al', 'comprar', 'un', 'libro', 'intenté', 'varias', 'veces']\n",
            "15 tokens de en medio:\n",
            "[]\n",
            "Últimos 15 tokens:\n",
            "['tardna', 'zapatlilas', 'serivcio', 'smartwatc', 'hmostraba', 'intentév', 'arias', 'naad', 'zapaitllas', 'mojad', 'acon', 'lbiro', 'pap', 'precios', 'ubió']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Vocabulario: {len(vocab)} tokens\")\n",
        "print(\"Primeros 15 tokens:\")\n",
        "print(f\"{top_n_tokens[:15]}\")\n",
        "print(\"15 tokens de en medio:\")\n",
        "print(f\"{top_n_tokens[1000:1015]}\")\n",
        "print(\"Últimos 15 tokens:\")\n",
        "print(f\"{top_n_tokens[-15:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa0668c",
      "metadata": {
        "id": "4fa0668c"
      },
      "source": [
        "Esta forma de exploración es para darnos una idea de las palabras más utilizadas en el corpus y nos dará un indicio de si la tokenización es adecuada o no. Vemos que tenemos algunos stop words, como artículos (el, la) y conectores (del, que). Para una tarea de clasificación de texto podríamos prescindir de estos pero para facilitar las cosas y ya que los demás tokens lucen bien, podemos preservarlos.\n",
        "\n",
        "Ahora veamos como convierte el tokenizador una oración muy sencilla:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "e4e68c73",
      "metadata": {
        "id": "e4e68c73",
        "outputId": "d8879e6a-347f-430d-8f27-32b57459f14a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "tokenized = tokenize_text(\"hola mundo\", max_length=8)\n",
        "tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008e6557",
      "metadata": {
        "id": "008e6557"
      },
      "source": [
        "Lo que obtenemos de vuelta son los ids de cada token según el vocabulario. Ahora algo importante que notamos aquí es el *padding*, durante el entrenamiento, queremos que las secuencias sean de tamaño fijo, para asi operar comodamente con matrices. Pero ya vimos que no todos los textos tienen la misma longitud. Entonces que hacer? para los que son más largos que una longitud dada simplemente cortamos, pero para los que son más cortos, debemos *rellenar* lo faltante con un *token especial de relleno o padding*. Y es justo lo que definimos allí, cuando la cadena es inferior a 8 **tokens**, entonces debemos hacer padding hasta que se cumplan los 8.\n",
        "\n",
        "Si queremos saber a que token exactamente hacen referencia estos ids, simplemente revisamos el vocabulario que hemos construido:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "d5049aaf",
      "metadata": {
        "id": "d5049aaf",
        "outputId": "309a75ec-354a-4ba7-e800-916935831ef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]', '[UNK]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "id_2_token = {v: k for k, v in vocab.items()}\n",
        "[id_2_token[token] for token in tokenized]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c18e433",
      "metadata": {
        "id": "9c18e433"
      },
      "source": [
        "Claramente vemos los 3 tokens como cadenas independientes (el padding se considera un token independiente).\n",
        "\n",
        "### Definiendo el dataset de pytorch\n",
        "Ahora podemos proceder a definir el dataset. Esto debería ser muy sencillo dado que nuestro dataset es pequeño y ya tenemos el tokenizador listo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "ca70e65a",
      "metadata": {
        "id": "ca70e65a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from typing import Tuple, Dict\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SpanishNewsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, dataset, seq_length: int = 512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataset = dataset\n",
        "        self.seq_length = seq_length\n",
        "        # Definimos estos dos mapas para facilitarnos la tarea\n",
        "        # de traducir de nombres de categoría a ids de categoría.\n",
        "        self.id_2_class_map = dict(enumerate(np.unique(dataset[:]['categoria'])))\n",
        "        self.class_2_id_map = {v: k for k, v in self.id_2_class_map.items()}\n",
        "        self.num_classes = len(self.id_2_class_map)\n",
        "\n",
        "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
        "        text, y = self.dataset[index]['comentario'], self.dataset[index]['categoria']\n",
        "        y = self.class_2_id_map[y]\n",
        "        data = {'id': torch.tensor(self.tokenizer(text, max_length=self.seq_length))}\n",
        "        data['y'] = torch.tensor(y)\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56de3493",
      "metadata": {
        "id": "56de3493"
      },
      "source": [
        "Ahora instanciaremos el dataset entero. Para este experimento, definiremos un tamaño máximo de secuencia de 2048 **tokens**. Que según nuestra intuición arriba, debería ser suficiente para la tarea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "15200ea5",
      "metadata": {
        "id": "15200ea5"
      },
      "outputs": [],
      "source": [
        "max_len = 512\n",
        "spanish_news_dataset = SpanishNewsDataset(tokenize_text, dataset, seq_length=max_len)\n",
        "assert len(spanish_news_dataset) == len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "611d4b0b",
      "metadata": {
        "id": "611d4b0b"
      },
      "source": [
        "Y luego, procedemos a hacer el train-val-test split y crear los dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "6d7e9e1c",
      "metadata": {
        "id": "6d7e9e1c"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 4 if not IN_COLAB else 12\n",
        "train_dataset, val_dataset, test_dataset = random_split(spanish_news_dataset, lengths=[0.8, 0.1, 0.1])\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ac44796",
      "metadata": {
        "id": "5ac44796"
      },
      "source": [
        "## Definición del modelo LSTM\n",
        "\n",
        "Ahora vamos a configurar un módulo pytorch simple para este problema. Vamos ha utilizar los embeddings, que vendrían siendo los vectores de palabra. Pytorch nos ofrece una capa con la que directamente podemos entrenarlos a partir de los token ids obtenidos. El resto consistirá en invocar una capa LSTM seguida de una capa densa para la clasificación.\n",
        "\n",
        "Recordemos que las redes recurrentes como las LSTM por diseño enlazan todas las dimensiones del vector de entrada, formando así la secuencia, la estructura natural que necesitamos representar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "d38eb14b",
      "metadata": {
        "id": "d38eb14b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMBlock(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, _) = self.lstm(embedded)\n",
        "        return hidden[-1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32f68240",
      "metadata": {
        "id": "32f68240"
      },
      "source": [
        "### Definición del clasificador\n",
        "\n",
        "Finalmente, definimos el modelo en si. Este modelo constará de 3 capas:\n",
        "\n",
        "- La tokenización, tal como la definimos anteriormente.\n",
        "- El bloque LSTM, que acabamos de decinir.\n",
        "- Una capa densa adicional que servirá como clasificador de aquello que nos entregue la capa del transformer.\n",
        "\n",
        "Como este es un LightningModule, aquí definiremos el resto de funciones utilitarias para el entrenamiento de la tarea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "1d724f85",
      "metadata": {
        "id": "1d724f85",
        "outputId": "063c38f4-f295-457b-92de-31ce371d52be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720,
          "referenced_widgets": [
            "218ac8153f954058b81739134de6465f",
            "14cde806cf2c4a22b8b21b409cf7482c",
            "3261efeb2e644db9b7ce94ff7a6717c1",
            "f8a4cef5dcbc487b92d4ec13db4ab668",
            "97091d736ccd41879c3e0d4ebb035764",
            "6365c9eef1af46a9a051b509be6059ed",
            "0bf42c884453444ebc5e6d38f94ec3c1",
            "9adeac374d914d678332ffc714d6db49",
            "39789921577a4247803357c094dc6b40",
            "be75b53f12654086a52f34e99d40224b",
            "fe84b402965540d0936ab3d9842de636"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name       | Type               | Params | Mode \n",
            "----------------------------------------------------------\n",
            "0 | lstm       | LSTMBlock          | 410 K  | train\n",
            "1 | classifier | Sequential         | 198 K  | train\n",
            "2 | train_acc  | MulticlassAccuracy | 0      | train\n",
            "3 | val_acc    | MulticlassAccuracy | 0      | train\n",
            "4 | test_acc   | MulticlassAccuracy | 0      | train\n",
            "----------------------------------------------------------\n",
            "608 K     Trainable params\n",
            "0         Non-trainable params\n",
            "608 K     Total params\n",
            "2.434     Total estimated model params size (MB)\n",
            "16        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "218ac8153f954058b81739134de6465f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "could not create a primitive descriptor for an LSTM forward propagation primitive",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2077137122.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"16-mixed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         )\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         )\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2077137122.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2077137122.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1529012800.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             result = _VF.lstm(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                 \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: could not create a primitive descriptor for an LSTM forward propagation primitive"
          ]
        }
      ],
      "source": [
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "class SpanishNewsClassifierWithLSTM(LightningModule):\n",
        "\n",
        "    def __init__(self, vocab_size: int, num_classes: int, emb_dim: int, hidden_dim: int = 128):\n",
        "        super(SpanishNewsClassifierWithLSTM, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.lstm = LSTMBlock(vocab_size, emb_dim, hidden_dim, num_classes)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(hidden_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.train_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "        self.val_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "        self.test_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.lstm(x)\n",
        "        return self.classifier(embeddings)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch['input_ids'], batch['y']\n",
        "        # print(f\"\\nbatch-idx: {batch_idx}\")\n",
        "        # print(f\"shape of x: {x.shape}\")\n",
        "        # print(torch.max(x, dim=0))\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.train_acc(y_hat, y)\n",
        "        self.log('train-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log('train-acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        x, y = batch['id'], batch['y']\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        self.val_acc(y_hat, y)\n",
        "        self.log('val-loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log('val-acc', self.val_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        x, y = batch['id'], batch['y']\n",
        "        y_hat = self(x)\n",
        "        self.test_acc(y_hat, y)\n",
        "        self.log('test-acc', self.test_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
        "\n",
        "\n",
        "    def predict_step(self, batch):\n",
        "        x = batch['id']\n",
        "        return self(x)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer =  torch.optim.AdamW(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "model = SpanishNewsClassifierWithLSTM(vocab_size=len(vocab) + 1, num_classes=spanish_news_dataset.num_classes, emb_dim=256)\n",
        "\n",
        "tb_logger = TensorBoardLogger('tb_logs', name='LSTMClassifier')\n",
        "callbacks=[EarlyStopping(monitor='train-loss', patience=3, mode='min')]\n",
        "trainer = Trainer(max_epochs=10, devices=1, logger=tb_logger, callbacks=callbacks, precision=\"16-mixed\")\n",
        "\n",
        "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7312d7c",
      "metadata": {
        "id": "b7312d7c"
      },
      "source": [
        "Observemos el proceso de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f9b20e",
      "metadata": {
        "id": "65f9b20e"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6dc01e",
      "metadata": {
        "id": "8e6dc01e",
        "outputId": "f4086caa-6a49-4780-a301-0228069c918d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-42fd403edfc8a0e6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-42fd403edfc8a0e6\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir tb_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5dd09d6",
      "metadata": {
        "id": "f5dd09d6"
      },
      "source": [
        "Y como es de esperarse, realizaremos la validación contra el conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d52f43",
      "metadata": {
        "id": "a5d52f43",
        "outputId": "1098e51d-c1ed-4533-9e22-35d467cd476e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0: 100%|██████████| 255/255 [00:01<00:00, 237.12it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test-acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9029411673545837     </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test-acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9029411673545837    \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'test-acc': 0.9029411673545837}]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "trainer.test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad06d232",
      "metadata": {
        "id": "ad06d232"
      },
      "source": [
        "### Haciendo predicciones\n",
        "\n",
        "Finalmente, vamos a hacer uso del modelo y ver que tan bueno es para la clasificación de noticias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff1c989e",
      "metadata": {
        "id": "ff1c989e",
        "outputId": "dff49068-8f68-4d54-be20-d41aa1db2c9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|██████████| 255/255 [00:01<00:00, 219.44it/s]\n"
          ]
        }
      ],
      "source": [
        "predictions = trainer.predict(model, test_loader)\n",
        "predictions = torch.cat(predictions, dim=0)\n",
        "predictions = torch.argmax(predictions, dim=-1)\n",
        "predictions = [spanish_news_dataset.id_2_class_map[pred] for pred in predictions.numpy()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdacb5c",
      "metadata": {
        "id": "8bdacb5c",
        "outputId": "848bdc2b-cf67-44f1-d177-0d1bec5adc2b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tokens_string</th>\n",
              "      <th>categoría</th>\n",
              "      <th>predicción</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6734</th>\n",
              "      <td>El Gobierno tiene un plan alternativo para ase...</td>\n",
              "      <td>[5, 3270, 1058, 102, 1737, 4007, 240, 834, 10,...</td>\n",
              "      <td>el gobierno tiene un plan alternativo para ase...</td>\n",
              "      <td>politics</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1905</th>\n",
              "      <td>Prevalencia de hipotiroidismo subclínico y su ...</td>\n",
              "      <td>[34012, 12, 1, 1, 36, 46, 703, 58, 1, 36, 1652...</td>\n",
              "      <td>prevalencia de [UNK] [UNK] y su relación con [...</td>\n",
              "      <td>medicine</td>\n",
              "      <td>medicine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3049</th>\n",
              "      <td>El Sol, la fuente inagotable de energía que da...</td>\n",
              "      <td>[5, 14608, 14, 2248, 10396, 12, 2641, 10, 246,...</td>\n",
              "      <td>el sol la fuente inagotable de energía que da ...</td>\n",
              "      <td>astronomy</td>\n",
              "      <td>astronomy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1772</th>\n",
              "      <td>Mondor en la mama. A propósito de un casoLa en...</td>\n",
              "      <td>[1, 54, 14, 1, 40, 554, 12, 102, 1, 1652, 12, ...</td>\n",
              "      <td>[UNK] en la [UNK] a propósito de un [UNK] enfe...</td>\n",
              "      <td>medicine</td>\n",
              "      <td>medicine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6980</th>\n",
              "      <td>Navantia acaba de firmar con la Marina de Noru...</td>\n",
              "      <td>[1, 11, 12, 23970, 58, 14, 27045, 12, 27768, 1...</td>\n",
              "      <td>[UNK] acaba de firmar con la marina de noruega...</td>\n",
              "      <td>military</td>\n",
              "      <td>military</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1589</th>\n",
              "      <td>Tienen una función parecida a los relojes o la...</td>\n",
              "      <td>[440, 19, 2427, 14721, 40, 34, 14066, 96, 77, ...</td>\n",
              "      <td>tienen una función parecida a los relojes o la...</td>\n",
              "      <td>tech</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>En última instancia, cuando se apagan las luce...</td>\n",
              "      <td>[54, 1177, 5104, 206, 17, 16158, 77, 3867, 12,...</td>\n",
              "      <td>en última instancia cuando se apagan las luces...</td>\n",
              "      <td>play</td>\n",
              "      <td>play</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7356</th>\n",
              "      <td>La compañía francesa Escape International ha p...</td>\n",
              "      <td>[14, 1151, 2772, 7008, 29665, 260, 1048, 54, 1...</td>\n",
              "      <td>la compañía francesa escape international ha p...</td>\n",
              "      <td>military</td>\n",
              "      <td>military</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6763</th>\n",
              "      <td>El Gobierno busca que Bruselas apruebe un impu...</td>\n",
              "      <td>[5, 3270, 232, 10, 14032, 36279, 102, 15003, 2...</td>\n",
              "      <td>el gobierno busca que bruselas apruebe un impu...</td>\n",
              "      <td>politics</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3341</th>\n",
              "      <td>comentarios 44Cybertron, el planeta de los Tra...</td>\n",
              "      <td>[7493, 1, 5, 3124, 12, 34, 8114, 5, 29619, 12,...</td>\n",
              "      <td>comentarios [UNK] el planeta de los transforme...</td>\n",
              "      <td>astronomy</td>\n",
              "      <td>astronomy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7826</th>\n",
              "      <td>La palabra bienquerencia es poco usada, pero e...</td>\n",
              "      <td>[14, 203, 1, 4, 610, 32326, 403, 198, 11659, 2...</td>\n",
              "      <td>la palabra [UNK] es poco usada pero existe ind...</td>\n",
              "      <td>religion</td>\n",
              "      <td>religion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8578</th>\n",
              "      <td>Segunda victoria en el WRC para el finlandés E...</td>\n",
              "      <td>[1090, 3982, 54, 5, 1, 240, 5, 19053, 1, 1, 10...</td>\n",
              "      <td>segunda victoria en el [UNK] para el finlandés...</td>\n",
              "      <td>motor</td>\n",
              "      <td>motor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3456</th>\n",
              "      <td>No hay un equipo más desgraciado que el Almerí...</td>\n",
              "      <td>[196, 124, 102, 4877, 130, 20027, 10, 5, 23674...</td>\n",
              "      <td>no hay un equipo más desgraciado que el almerí...</td>\n",
              "      <td>sport</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5659</th>\n",
              "      <td>Más allá de las cápsulas de café hay un mundo ...</td>\n",
              "      <td>[130, 219, 12, 77, 32819, 12, 10027, 124, 102,...</td>\n",
              "      <td>más allá de las cápsulas de café hay un mundo ...</td>\n",
              "      <td>alimentation</td>\n",
              "      <td>alimentation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2184</th>\n",
              "      <td>Evolucion de la vida sexual de la mujer. Fisio...</td>\n",
              "      <td>[1, 12, 14, 195, 7773, 12, 14, 749, 1, 12, 14,...</td>\n",
              "      <td>[UNK] de la vida sexual de la mujer [UNK] de l...</td>\n",
              "      <td>medicine</td>\n",
              "      <td>medicine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  texto  \\\n",
              "6734  El Gobierno tiene un plan alternativo para ase...   \n",
              "1905  Prevalencia de hipotiroidismo subclínico y su ...   \n",
              "3049  El Sol, la fuente inagotable de energía que da...   \n",
              "1772  Mondor en la mama. A propósito de un casoLa en...   \n",
              "6980  Navantia acaba de firmar con la Marina de Noru...   \n",
              "1589  Tienen una función parecida a los relojes o la...   \n",
              "240   En última instancia, cuando se apagan las luce...   \n",
              "7356  La compañía francesa Escape International ha p...   \n",
              "6763  El Gobierno busca que Bruselas apruebe un impu...   \n",
              "3341  comentarios 44Cybertron, el planeta de los Tra...   \n",
              "7826  La palabra bienquerencia es poco usada, pero e...   \n",
              "8578  Segunda victoria en el WRC para el finlandés E...   \n",
              "3456  No hay un equipo más desgraciado que el Almerí...   \n",
              "5659  Más allá de las cápsulas de café hay un mundo ...   \n",
              "2184  Evolucion de la vida sexual de la mujer. Fisio...   \n",
              "\n",
              "                                                 tokens  \\\n",
              "6734  [5, 3270, 1058, 102, 1737, 4007, 240, 834, 10,...   \n",
              "1905  [34012, 12, 1, 1, 36, 46, 703, 58, 1, 36, 1652...   \n",
              "3049  [5, 14608, 14, 2248, 10396, 12, 2641, 10, 246,...   \n",
              "1772  [1, 54, 14, 1, 40, 554, 12, 102, 1, 1652, 12, ...   \n",
              "6980  [1, 11, 12, 23970, 58, 14, 27045, 12, 27768, 1...   \n",
              "1589  [440, 19, 2427, 14721, 40, 34, 14066, 96, 77, ...   \n",
              "240   [54, 1177, 5104, 206, 17, 16158, 77, 3867, 12,...   \n",
              "7356  [14, 1151, 2772, 7008, 29665, 260, 1048, 54, 1...   \n",
              "6763  [5, 3270, 232, 10, 14032, 36279, 102, 15003, 2...   \n",
              "3341  [7493, 1, 5, 3124, 12, 34, 8114, 5, 29619, 12,...   \n",
              "7826  [14, 203, 1, 4, 610, 32326, 403, 198, 11659, 2...   \n",
              "8578  [1090, 3982, 54, 5, 1, 240, 5, 19053, 1, 1, 10...   \n",
              "3456  [196, 124, 102, 4877, 130, 20027, 10, 5, 23674...   \n",
              "5659  [130, 219, 12, 77, 32819, 12, 10027, 124, 102,...   \n",
              "2184  [1, 12, 14, 195, 7773, 12, 14, 749, 1, 12, 14,...   \n",
              "\n",
              "                                          tokens_string     categoría  \\\n",
              "6734  el gobierno tiene un plan alternativo para ase...      politics   \n",
              "1905  prevalencia de [UNK] [UNK] y su relación con [...      medicine   \n",
              "3049  el sol la fuente inagotable de energía que da ...     astronomy   \n",
              "1772  [UNK] en la [UNK] a propósito de un [UNK] enfe...      medicine   \n",
              "6980  [UNK] acaba de firmar con la marina de noruega...      military   \n",
              "1589  tienen una función parecida a los relojes o la...          tech   \n",
              "240   en última instancia cuando se apagan las luces...          play   \n",
              "7356  la compañía francesa escape international ha p...      military   \n",
              "6763  el gobierno busca que bruselas apruebe un impu...      politics   \n",
              "3341  comentarios [UNK] el planeta de los transforme...     astronomy   \n",
              "7826  la palabra [UNK] es poco usada pero existe ind...      religion   \n",
              "8578  segunda victoria en el [UNK] para el finlandés...         motor   \n",
              "3456  no hay un equipo más desgraciado que el almerí...         sport   \n",
              "5659  más allá de las cápsulas de café hay un mundo ...  alimentation   \n",
              "2184  [UNK] de la vida sexual de la mujer [UNK] de l...      medicine   \n",
              "\n",
              "        predicción  \n",
              "6734      politics  \n",
              "1905      medicine  \n",
              "3049     astronomy  \n",
              "1772      medicine  \n",
              "6980      military  \n",
              "1589          tech  \n",
              "240           play  \n",
              "7356      military  \n",
              "6763      politics  \n",
              "3341     astronomy  \n",
              "7826      religion  \n",
              "8578         motor  \n",
              "3456         sport  \n",
              "5659  alimentation  \n",
              "2184      medicine  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_indices = test_dataset.indices\n",
        "df = pd.DataFrame(data={\n",
        "    \"texto\": dataset[test_indices]['text'],\n",
        "    \"tokens\": [tokenize_text(v) for v in dataset[test_indices]['text']],\n",
        "    \"categoría\": dataset[test_indices]['category'],\n",
        "    'predicción': predictions\n",
        "}, index=test_indices)\n",
        "\n",
        "df['tokens_string'] = df.tokens.apply(lambda t: ' '.join([id_2_token[i] for i in t]))\n",
        "df = df[[\"texto\", \"tokens\", \"tokens_string\", \"categoría\", \"predicción\"]]\n",
        "df.style.set_table_styles(\n",
        "    [\n",
        "        {'selector': 'td', 'props': [('word-wrap', 'break-word')]}\n",
        "    ]\n",
        ")\n",
        "df.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c5085d5",
      "metadata": {
        "id": "9c5085d5",
        "outputId": "15e4bf3a-00e0-4245-f3de-84b86f44b40b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tokens_string</th>\n",
              "      <th>categoría</th>\n",
              "      <th>predicción</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1120</th>\n",
              "      <td>Pues parece que la policía holandesa se precip...</td>\n",
              "      <td>[2020, 491, 10, 14, 1497, 38011, 17, 38012, 10...</td>\n",
              "      <td>pues parece que la policía holandesa se precip...</td>\n",
              "      <td>tech</td>\n",
              "      <td>play</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7473</th>\n",
              "      <td>El nuevo Consejo de Administración de Fincanti...</td>\n",
              "      <td>[5, 8, 17379, 12, 444, 12, 1, 1, 29649, 45, 14...</td>\n",
              "      <td>el nuevo consejo de administración de [UNK] [U...</td>\n",
              "      <td>military</td>\n",
              "      <td>economy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>Fuente de la imagen, OtherJune Williams creció...</td>\n",
              "      <td>[2248, 12, 14, 564, 1, 10953, 5680, 57, 40, 34...</td>\n",
              "      <td>fuente de la imagen [UNK] williams creció junt...</td>\n",
              "      <td>astronomy</td>\n",
              "      <td>religion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4142</th>\n",
              "      <td>El Elche espera a Óscar Plano. El delantero ma...</td>\n",
              "      <td>[5, 36649, 4040, 40, 2947, 1846, 5, 1, 8710, 3...</td>\n",
              "      <td>el elche espera a óscar plano el [UNK] madrile...</td>\n",
              "      <td>sport</td>\n",
              "      <td>economy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>689</th>\n",
              "      <td>Algunos de los monumentos más famosos del Rein...</td>\n",
              "      <td>[501, 12, 34, 23928, 130, 1018, 7, 2125, 8269,...</td>\n",
              "      <td>algunos de los monumentos más famosos del rein...</td>\n",
              "      <td>play</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5835</th>\n",
              "      <td>Hay anuncios que irremediablemente se quedan a...</td>\n",
              "      <td>[124, 11554, 10, 12505, 17, 1895, 12251, 54, 2...</td>\n",
              "      <td>hay anuncios que irremediablemente se quedan a...</td>\n",
              "      <td>alimentation</td>\n",
              "      <td>astronomy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7368</th>\n",
              "      <td>Navantia junto a otras 5 grandes empresas, Rep...</td>\n",
              "      <td>[1, 57, 40, 2255, 901, 8594, 1, 1, 14784, 3176...</td>\n",
              "      <td>[UNK] junto a otras grandes empresas [UNK] [UN...</td>\n",
              "      <td>military</td>\n",
              "      <td>motor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9001</th>\n",
              "      <td>La versión más accesible del Lucid Air llega p...</td>\n",
              "      <td>[14, 2049, 130, 14266, 7, 1, 7550, 333, 45, 31...</td>\n",
              "      <td>la versión más accesible del [UNK] air llega p...</td>\n",
              "      <td>motor</td>\n",
              "      <td>economy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7846</th>\n",
              "      <td>Pedro Sánchez ha regalado toda una ley de adoc...</td>\n",
              "      <td>[399, 9554, 260, 19502, 89, 19, 2565, 12, 1, 5...</td>\n",
              "      <td>pedro sánchez ha regalado toda una ley de [UNK...</td>\n",
              "      <td>religion</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8103</th>\n",
              "      <td>Es algo que está sorprendiendo en la red. Mill...</td>\n",
              "      <td>[4, 559, 10, 482, 24341, 54, 14, 1063, 1059, 1...</td>\n",
              "      <td>es algo que está sorprendiendo en la red millo...</td>\n",
              "      <td>religion</td>\n",
              "      <td>astronomy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9900</th>\n",
              "      <td>La Edad Media no fue la época maloliente y suc...</td>\n",
              "      <td>[14, 7466, 4151, 196, 395, 14, 1629, 1, 36, 47...</td>\n",
              "      <td>la edad media no fue la época [UNK] y sucia qu...</td>\n",
              "      <td>economy</td>\n",
              "      <td>astronomy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>786</th>\n",
              "      <td>La última generación de videoconsolas lanzadas...</td>\n",
              "      <td>[14, 1177, 2224, 12, 30807, 21743, 269, 1481, ...</td>\n",
              "      <td>la última generación de videoconsolas lanzadas...</td>\n",
              "      <td>play</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9576</th>\n",
              "      <td>La necesidad de energía está presente en práct...</td>\n",
              "      <td>[14, 2497, 12, 2641, 482, 388, 54, 4057, 201, ...</td>\n",
              "      <td>la necesidad de energía está presente en práct...</td>\n",
              "      <td>economy</td>\n",
              "      <td>fashion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2098</th>\n",
              "      <td>¿Qué es la farmacología? Ciencia que estudia l...</td>\n",
              "      <td>[1183, 4, 14, 1, 4182, 10, 38159, 34, 11561, 1...</td>\n",
              "      <td>qué es la [UNK] ciencia que estudia los fármac...</td>\n",
              "      <td>medicine</td>\n",
              "      <td>military</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7888</th>\n",
              "      <td>El Ayuntamiento de Palma ha colocado este mart...</td>\n",
              "      <td>[5, 4852, 12, 9239, 260, 15095, 55, 9247, 14, ...</td>\n",
              "      <td>el ayuntamiento de palma ha colocado este mart...</td>\n",
              "      <td>religion</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  texto  \\\n",
              "1120  Pues parece que la policía holandesa se precip...   \n",
              "7473  El nuevo Consejo de Administración de Fincanti...   \n",
              "2765  Fuente de la imagen, OtherJune Williams creció...   \n",
              "4142  El Elche espera a Óscar Plano. El delantero ma...   \n",
              "689   Algunos de los monumentos más famosos del Rein...   \n",
              "5835  Hay anuncios que irremediablemente se quedan a...   \n",
              "7368  Navantia junto a otras 5 grandes empresas, Rep...   \n",
              "9001  La versión más accesible del Lucid Air llega p...   \n",
              "7846  Pedro Sánchez ha regalado toda una ley de adoc...   \n",
              "8103  Es algo que está sorprendiendo en la red. Mill...   \n",
              "9900  La Edad Media no fue la época maloliente y suc...   \n",
              "786   La última generación de videoconsolas lanzadas...   \n",
              "9576  La necesidad de energía está presente en práct...   \n",
              "2098  ¿Qué es la farmacología? Ciencia que estudia l...   \n",
              "7888  El Ayuntamiento de Palma ha colocado este mart...   \n",
              "\n",
              "                                                 tokens  \\\n",
              "1120  [2020, 491, 10, 14, 1497, 38011, 17, 38012, 10...   \n",
              "7473  [5, 8, 17379, 12, 444, 12, 1, 1, 29649, 45, 14...   \n",
              "2765  [2248, 12, 14, 564, 1, 10953, 5680, 57, 40, 34...   \n",
              "4142  [5, 36649, 4040, 40, 2947, 1846, 5, 1, 8710, 3...   \n",
              "689   [501, 12, 34, 23928, 130, 1018, 7, 2125, 8269,...   \n",
              "5835  [124, 11554, 10, 12505, 17, 1895, 12251, 54, 2...   \n",
              "7368  [1, 57, 40, 2255, 901, 8594, 1, 1, 14784, 3176...   \n",
              "9001  [14, 2049, 130, 14266, 7, 1, 7550, 333, 45, 31...   \n",
              "7846  [399, 9554, 260, 19502, 89, 19, 2565, 12, 1, 5...   \n",
              "8103  [4, 559, 10, 482, 24341, 54, 14, 1063, 1059, 1...   \n",
              "9900  [14, 7466, 4151, 196, 395, 14, 1629, 1, 36, 47...   \n",
              "786   [14, 1177, 2224, 12, 30807, 21743, 269, 1481, ...   \n",
              "9576  [14, 2497, 12, 2641, 482, 388, 54, 4057, 201, ...   \n",
              "2098  [1183, 4, 14, 1, 4182, 10, 38159, 34, 11561, 1...   \n",
              "7888  [5, 4852, 12, 9239, 260, 15095, 55, 9247, 14, ...   \n",
              "\n",
              "                                          tokens_string     categoría  \\\n",
              "1120  pues parece que la policía holandesa se precip...          tech   \n",
              "7473  el nuevo consejo de administración de [UNK] [U...      military   \n",
              "2765  fuente de la imagen [UNK] williams creció junt...     astronomy   \n",
              "4142  el elche espera a óscar plano el [UNK] madrile...         sport   \n",
              "689   algunos de los monumentos más famosos del rein...          play   \n",
              "5835  hay anuncios que irremediablemente se quedan a...  alimentation   \n",
              "7368  [UNK] junto a otras grandes empresas [UNK] [UN...      military   \n",
              "9001  la versión más accesible del [UNK] air llega p...         motor   \n",
              "7846  pedro sánchez ha regalado toda una ley de [UNK...      religion   \n",
              "8103  es algo que está sorprendiendo en la red millo...      religion   \n",
              "9900  la edad media no fue la época [UNK] y sucia qu...       economy   \n",
              "786   la última generación de videoconsolas lanzadas...          play   \n",
              "9576  la necesidad de energía está presente en práct...       economy   \n",
              "2098  qué es la [UNK] ciencia que estudia los fármac...      medicine   \n",
              "7888  el ayuntamiento de palma ha colocado este mart...      religion   \n",
              "\n",
              "     predicción  \n",
              "1120       play  \n",
              "7473    economy  \n",
              "2765   religion  \n",
              "4142    economy  \n",
              "689        tech  \n",
              "5835  astronomy  \n",
              "7368      motor  \n",
              "9001    economy  \n",
              "7846   politics  \n",
              "8103  astronomy  \n",
              "9900  astronomy  \n",
              "786        tech  \n",
              "9576    fashion  \n",
              "2098   military  \n",
              "7888   politics  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "errors = df[df['categoría'] != df['predicción']]\n",
        "errors.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88103e5e",
      "metadata": {
        "id": "88103e5e"
      },
      "source": [
        "## Conclusiones\n",
        "\n",
        "- En este caso tenemos una tarea de clasificación de texto de múltiples clases.\n",
        "- Estamos usando un bloque LSTM como featurizer, es decir lo usamos para extraer features de las secuencias de entrada con las cuales harémos predicciones luego.\n",
        "- Nótese que de las capas LSTM, solo nos interesa la última, ya que esta recupera todas las operaciones enalazadas anteriores.\n",
        "- Observamos que el modelo toma su tiempo en entrenar, esto es natural debido al diseño de las LSTM, donde por cada paso de tiempo se debe computar un gradiente, por lo que el computo es mucho mayor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e73526f",
      "metadata": {
        "id": "4e73526f"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "icesi-nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e667ffdde87c44e2b47e9122bd72ebba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a144b0f65a6d48449b4b8dfca55d48ae",
              "IPY_MODEL_d14fadcf268c44cc979bfd3b81ccb4dd",
              "IPY_MODEL_0ea48a41f0c64ddb8713ec95c7fd9afe"
            ],
            "layout": "IPY_MODEL_5ed5416f52c64c2ba004117116afbb94"
          }
        },
        "a144b0f65a6d48449b4b8dfca55d48ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f391260a25940f7bd12504517d095f6",
            "placeholder": "​",
            "style": "IPY_MODEL_66b4f091239349bfacf1e28a39234486",
            "value": "Map: 100%"
          }
        },
        "d14fadcf268c44cc979bfd3b81ccb4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4091367ab02544acb7b79a59e6215087",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bb0996639f4407d87bcf3d5769a3726",
            "value": 2000
          }
        },
        "0ea48a41f0c64ddb8713ec95c7fd9afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67953ebe4e434630a3668706284889ae",
            "placeholder": "​",
            "style": "IPY_MODEL_90d9c7c2328d48af8ee00a5d43da2ea0",
            "value": " 2000/2000 [00:00&lt;00:00, 5943.42 examples/s]"
          }
        },
        "5ed5416f52c64c2ba004117116afbb94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f391260a25940f7bd12504517d095f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b4f091239349bfacf1e28a39234486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4091367ab02544acb7b79a59e6215087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bb0996639f4407d87bcf3d5769a3726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67953ebe4e434630a3668706284889ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90d9c7c2328d48af8ee00a5d43da2ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "218ac8153f954058b81739134de6465f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14cde806cf2c4a22b8b21b409cf7482c",
              "IPY_MODEL_3261efeb2e644db9b7ce94ff7a6717c1",
              "IPY_MODEL_f8a4cef5dcbc487b92d4ec13db4ab668"
            ],
            "layout": "IPY_MODEL_97091d736ccd41879c3e0d4ebb035764"
          }
        },
        "14cde806cf2c4a22b8b21b409cf7482c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6365c9eef1af46a9a051b509be6059ed",
            "placeholder": "​",
            "style": "IPY_MODEL_0bf42c884453444ebc5e6d38f94ec3c1",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "3261efeb2e644db9b7ce94ff7a6717c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9adeac374d914d678332ffc714d6db49",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39789921577a4247803357c094dc6b40",
            "value": 0
          }
        },
        "f8a4cef5dcbc487b92d4ec13db4ab668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be75b53f12654086a52f34e99d40224b",
            "placeholder": "​",
            "style": "IPY_MODEL_fe84b402965540d0936ab3d9842de636",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "97091d736ccd41879c3e0d4ebb035764": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "6365c9eef1af46a9a051b509be6059ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bf42c884453444ebc5e6d38f94ec3c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9adeac374d914d678332ffc714d6db49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39789921577a4247803357c094dc6b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be75b53f12654086a52f34e99d40224b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe84b402965540d0936ab3d9842de636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}